{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Path to folder with necessary files\n",
    "path = 'YOUR/PATH/'\n",
    "\n",
    "print('loading text...')\n",
    "# load review data\n",
    "\n",
    "reviews = pd.read_csv(path+\"test_cleaned_reviews.csv\")\n",
    "reviews = reviews[reviews.review.str.contains('notebook|computer', regex=True)]\n",
    "\n",
    "reviews_text = reviews.review.values.tolist()\n",
    "reviews_text = str(reviews_text)\n",
    "\n",
    "processed_reviews = reviews_text.lower()\n",
    "processed_reviews = re.sub('[^a-zA-Z]', ' ', processed_reviews )\n",
    "processed_reviews = re.sub(r'\\s+', ' ', processed_reviews)\n",
    "\n",
    "\n",
    "print('tokenizing...')\n",
    "# tokenize sentences\n",
    "\n",
    "all_sentences = nltk.sent_tokenize(processed_reviews)\n",
    "\n",
    "data_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "\n",
    "print('bigrams, stopwords and lemmatizations...')\n",
    "# Build the bigram model and function\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "\n",
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "bigram = gensim.models.Phrases(data_words_nostops, min_count=2, threshold=10)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy \n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "nlp.max_length = 106568194\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "\n",
    "print('fitting word2vec...')\n",
    "# Fit word2vec\n",
    "word2vec = Word2Vec(data_lemmatized, min_count=2)\n",
    "X = word2vec[word2vec.wv.vocab]\n",
    "\n",
    "print('fitting KMeans...')\n",
    "# Fit KMeans\n",
    "kmeans = cluster.KMeans(n_clusters=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "print('running kmeans model on aspect sample...')\n",
    "# For loop on aspect df to cluster any words in word2vec vocabulary\n",
    "pd.options.mode.chained_assignment = None \n",
    "aspects = pd.read_csv(path+\"laptop_filtered_aspect_sample.csv\")\n",
    "aspects['cluster'] = np.nan\n",
    "\n",
    "for i in range(len(aspects)-1):\n",
    "    try:\n",
    "        pred = kmeans.predict(word2vec[aspects.aspect_name[i:i+1]])\n",
    "        aspects['cluster'][i] = pred\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print('saving dataframe with cluster data...')        \n",
    "aspects.to_csv(path+'clustered_aspects.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
